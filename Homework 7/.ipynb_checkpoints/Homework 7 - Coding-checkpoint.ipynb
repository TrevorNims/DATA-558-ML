{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "urban-advertising",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "## (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-accuracy",
   "metadata": {},
   "source": [
    "With our objective function given by:\n",
    "\n",
    "$$F(\\alpha) = \\frac{1}{n}\\sum_{i=1}^{n}[\\text{max}(0, 1-y_i(K\\alpha)_i)]^2+\\lambda\\alpha^TK\\alpha$$\n",
    "\n",
    "Its gradient is given by:\n",
    "\n",
    "$$\n",
    "\\nabla_\\alpha F(\\alpha) = \\begin{cases}\n",
    "            \\frac{1}{n}\\sum_{i=1}^{n}\\big(-2y_i(1-y_i(\\tilde{x}^T_i\\alpha)_i)\\cdot \\tilde{x}_i\\big) + 2\\lambda K \\alpha & y_i(\\tilde{x}^T_i\\alpha) < 1 \\\\\\\\\\\\\n",
    "            2\\lambda K \\alpha  \\quad \\text{otherwise}\n",
    "            \\end{cases}\n",
    "$$\n",
    "Where $\\tilde{x}_i$ is given by:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "k(x_i, x_1) \\\\\n",
    "\\vdots \\\\\n",
    "k(x_i, x_n)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "And $K$ is given by:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\tilde{x}^T_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\tilde{x}^T_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Despite being a piecewise function, our objective is differentiable at $y_i(\\tilde{x}^T_i\\alpha) = 1$ because both the objective and the gradient agree with themselves at this point (at this point, the objective's value is $\\lambda\\alpha^TK\\alpha$ in both cases and the its gradient's value is $2\\lambda K \\alpha$ in both cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "breeding-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-detector",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "attractive-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define polynomial kernel function\n",
    "\n",
    "def kern_poly(x, y, offset, order):\n",
    "    return (np.dot(x.T, y)+offset)**order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-replacement",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "revised-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to compute gram matrix\n",
    "\n",
    "def compute_gram(kernel, X, offset=0, order=1):\n",
    "    res = np.zeros((X.shape[0], X.shape[0]))\n",
    "    for i, x_i in enumerate(X):\n",
    "        for j, x_j in enumerate(X):\n",
    "            res[i, j] = kernel(x_i, x_j, offset, order)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-address",
   "metadata": {},
   "source": [
    "## (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "adverse-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to compute vector of kernel evaluations\n",
    "\n",
    "def kernel_eval(kernel, X, x_star, offset=0, order=1):\n",
    "    res = np.zeros(X.shape[0])\n",
    "    for i, x in enumerate(X):\n",
    "        res[i] = kernel(x, x_star, offset, order)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-whale",
   "metadata": {},
   "source": [
    "## (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dimensional-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective function\n",
    "\n",
    "def objective_fun(alpha, gram, y, lambda_):\n",
    "    n = len(y)\n",
    "    run_sum = 0\n",
    "    for i in range(n):\n",
    "        temp = 1-y[i]*np.dot(gram[i], alpha)\n",
    "        if temp > 0:\n",
    "            run_sum += temp**2\n",
    "    return 1/n*run_sum+lambda_*np.dot(np.dot(alpha.T, gram), alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "electronic-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to compute gradient\n",
    "\n",
    "def compute_grad(alpha, gram, y, lambda_):\n",
    "    n = len(y)\n",
    "    run_sum = np.zeros(alpha.shape[0])\n",
    "    for i in range(n):\n",
    "        temp = 1-y[i]*np.dot(gram[i], alpha)\n",
    "        if temp > 0:\n",
    "            run_sum += y[i]*(temp)*gram[i].T\n",
    "    return -2/n*run_sum+2*lambda_*np.dot(gram, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "european-garbage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define backtracking function\n",
    "\n",
    "def backtracking(alpha, gram, y, lambda_, eta_init, decay_rate=0.8, prop_constant=0.5):\n",
    "    eta = eta_init\n",
    "    \n",
    "    def sufficient_decrease(eta):\n",
    "        grad = compute_grad(alpha, gram, y, lambda_)\n",
    "        return (objective_fun(alpha-eta*grad, gram, y, lambda_)-objective_fun(alpha, gram, y, lambda_)) <= \\\n",
    "                (prop_constant*eta*np.dot(grad.T, -grad))\n",
    "    \n",
    "    while not sufficient_decrease(eta):\n",
    "        eta *= decay_rate\n",
    "    return eta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "criminal-lebanon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fast grad descent algorithm with backtracking\n",
    "\n",
    "def fast_grad_algo(step_size, epsilon, gram, y, lambda_):\n",
    "    iterates = []\n",
    "    alpha = np.zeros(len(y))\n",
    "    theta = np.zeros(len(y))\n",
    "    grad = compute_grad(alpha, gram, y, lambda_)\n",
    "    steps = 0\n",
    "    while np.linalg.norm(grad) > epsilon:\n",
    "        step_size = backtracking(alpha, gram, y, lambda_, step_size)\n",
    "        new_alpha = theta - step_size*compute_grad(theta, gram, y, lambda_)\n",
    "        theta = new_alpha + steps/(steps+3)*(new_alpha-alpha)\n",
    "        iterates.append(new_alpha)\n",
    "        alpha = new_alpha\n",
    "        grad = compute_grad(alpha, gram, y, lambda_)\n",
    "        steps += 1\n",
    "    return iterates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-distinction",
   "metadata": {},
   "source": [
    "## (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "solar-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in dataset\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ignored-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate images from labels, vectorize images, normalize images\n",
    "\n",
    "X = np.array([np.ravel(img) for img in digits.images])\n",
    "y = digits.target\n",
    "X_normalized = np.array([img/np.linalg.norm(img) for img in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "analyzed-minutes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 80/20 train-test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-costs",
   "metadata": {},
   "source": [
    "## (g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "designing-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary label matrix\n",
    "\n",
    "num_digits = 10\n",
    "\n",
    "y_train_mat = np.full((len(y_train), num_digits), -1)\n",
    "for i in range(num_digits):\n",
    "    for j in range(len(y_train)):\n",
    "        if i == y_train[j]:\n",
    "            y_train_mat[j, i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "roman-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify hyper-parameters, compute gram matrix for training data\n",
    "\n",
    "lambda_ = 10.0\n",
    "eta_init = .01\n",
    "tolerance = 1*10**-2\n",
    "gram = compute_gram(kern_poly, X_train, offset=1, order=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "recovered-escape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for this iteration: -78.12407398223877 \n",
      "\n",
      "\n",
      "Elapsed time for this iteration: -110.47942399978638 \n",
      "\n",
      "\n",
      "Elapsed time for this iteration: -91.22537779808044 \n",
      "\n",
      "\n",
      "Elapsed time for this iteration: -104.63679099082947 \n",
      "\n",
      "\n",
      "Elapsed time for this iteration: -87.65658807754517 \n",
      "\n",
      "\n",
      "Elapsed time for this iteration: -92.82212686538696 \n",
      "\n",
      "\n",
      "Elapsed time for this iteration: -89.43778586387634 \n",
      "\n",
      "\n",
      "Elapsed time for this iteration: -92.74758720397949 \n",
      "\n",
      "\n",
      "Elapsed time for this iteration: -119.05746603012085 \n",
      "\n",
      "\n",
      "Elapsed time for this iteration: -110.23310804367065 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run fast gradient descent to develop a classifier for each of the \n",
    "# 10 digits (takes a long time!)\n",
    "\n",
    "iterates = []\n",
    "for i in range(num_digits):\n",
    "    start_time = time.time()\n",
    "    iterates.append(fast_grad_algo(eta_init, tolerance, gram, y_train_mat[:,i], lambda_))\n",
    "    end_time = time.time()\n",
    "    print('Elapsed time for this iteration:', end_time-start_time, 's\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "republican-officer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot objective function vs. iteration number for each classifier\n",
    "\n",
    "for i in range(num_digits):\n",
    "    obj_trace = [objective_fun(a, gram, y_train_mat[:,i], lambda_) for a in iterates[i]]\n",
    "    plt.plot(obj_trace)\n",
    "plt.xlabel('Step Number ($t$)')\n",
    "plt.ylabel(r'Value of Objective Function $F(\\alpha_t)$')\n",
    "plt.title(r'Value of $F(\\alpha_t)$ vs $t$ ($\\lambda=10.0$)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-fundamentals",
   "metadata": {},
   "source": [
    "## (h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "periodic-attack",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify lambda range and number of folds\n",
    "\n",
    "lambdas = np.logspace(-3, 1, 20)\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "intense-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-Fold CV function \n",
    "\n",
    "def k_fold_CV(X, y, lambdas, k):\n",
    "    results = np.zeros((len(lambdas), k))\n",
    "\n",
    "    for i, lambda_ in enumerate(lambdas):\n",
    "        print('\\tEvaluating lambda ' + str(i) + '...\\n')\n",
    "        for j in range(k):  \n",
    "            X_groups = np.array_split(X, k)\n",
    "            X_train = np.vstack([group for idx, group in enumerate(X_groups) if idx != j])\n",
    "            X_val = X_groups[j]\n",
    "            \n",
    "            y_groups = np.array_split(y, k)\n",
    "            y_train = np.hstack([group for idx, group in enumerate(y_groups) if idx != j])\n",
    "            y_val = y_groups[j]\n",
    "            \n",
    "            gram_train = compute_gram(kern_poly, X_train, offset=1, order=7)\n",
    "            gram_val = np.array([kernel_eval(kern_poly, X_train, x_star, offset=1, order=7) for x_star in X_val])\n",
    "            iterates = fast_grad_algo(.01, 1, gram_train, y_train, lambda_)\n",
    "            preds = np.dot(iterates[-1], gram_val.T) < 0\n",
    "            results[i,j] = np.mean(preds == (y_val < 0))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "widespread-private",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Cross-Validation on Model: 0\n",
      "\tEvaluating lambda 0...\n",
      "\n",
      "\tEvaluating lambda 1...\n",
      "\n",
      "\tEvaluating lambda 2...\n",
      "\n",
      "\tEvaluating lambda 3...\n",
      "\n",
      "\tEvaluating lambda 4...\n",
      "\n",
      "\tEvaluating lambda 5...\n",
      "\n",
      "\tEvaluating lambda 6...\n",
      "\n",
      "\tEvaluating lambda 7...\n",
      "\n",
      "\tEvaluating lambda 8...\n",
      "\n",
      "\tEvaluating lambda 9...\n",
      "\n",
      "\tEvaluating lambda 10...\n",
      "\n",
      "\tEvaluating lambda 11...\n",
      "\n",
      "\tEvaluating lambda 12...\n",
      "\n",
      "\tEvaluating lambda 13...\n",
      "\n",
      "\tEvaluating lambda 14...\n",
      "\n",
      "\tEvaluating lambda 15...\n",
      "\n",
      "\tEvaluating lambda 16...\n",
      "\n",
      "\tEvaluating lambda 17...\n",
      "\n",
      "\tEvaluating lambda 18...\n",
      "\n",
      "\tEvaluating lambda 19...\n",
      "\n",
      "Running Cross-Validation on Model: 1\n",
      "\tEvaluating lambda 0...\n",
      "\n",
      "\tEvaluating lambda 1...\n",
      "\n",
      "\tEvaluating lambda 2...\n",
      "\n",
      "\tEvaluating lambda 3...\n",
      "\n",
      "\tEvaluating lambda 4...\n",
      "\n",
      "\tEvaluating lambda 5...\n",
      "\n",
      "\tEvaluating lambda 6...\n",
      "\n",
      "\tEvaluating lambda 7...\n",
      "\n",
      "\tEvaluating lambda 8...\n",
      "\n",
      "\tEvaluating lambda 9...\n",
      "\n",
      "\tEvaluating lambda 10...\n",
      "\n",
      "\tEvaluating lambda 11...\n",
      "\n",
      "\tEvaluating lambda 12...\n",
      "\n",
      "\tEvaluating lambda 13...\n",
      "\n",
      "\tEvaluating lambda 14...\n",
      "\n",
      "\tEvaluating lambda 15...\n",
      "\n",
      "\tEvaluating lambda 16...\n",
      "\n",
      "\tEvaluating lambda 17...\n",
      "\n",
      "\tEvaluating lambda 18...\n",
      "\n",
      "\tEvaluating lambda 19...\n",
      "\n",
      "Running Cross-Validation on Model: 2\n",
      "\tEvaluating lambda 0...\n",
      "\n",
      "\tEvaluating lambda 1...\n",
      "\n",
      "\tEvaluating lambda 2...\n",
      "\n",
      "\tEvaluating lambda 3...\n",
      "\n",
      "\tEvaluating lambda 4...\n",
      "\n",
      "\tEvaluating lambda 5...\n",
      "\n",
      "\tEvaluating lambda 6...\n",
      "\n",
      "\tEvaluating lambda 7...\n",
      "\n",
      "\tEvaluating lambda 8...\n",
      "\n",
      "\tEvaluating lambda 9...\n",
      "\n",
      "\tEvaluating lambda 10...\n",
      "\n",
      "\tEvaluating lambda 11...\n",
      "\n",
      "\tEvaluating lambda 12...\n",
      "\n",
      "\tEvaluating lambda 13...\n",
      "\n",
      "\tEvaluating lambda 14...\n",
      "\n",
      "\tEvaluating lambda 15...\n",
      "\n",
      "\tEvaluating lambda 16...\n",
      "\n",
      "\tEvaluating lambda 17...\n",
      "\n",
      "\tEvaluating lambda 18...\n",
      "\n",
      "\tEvaluating lambda 19...\n",
      "\n",
      "Running Cross-Validation on Model: 3\n",
      "\tEvaluating lambda 0...\n",
      "\n",
      "\tEvaluating lambda 1...\n",
      "\n",
      "\tEvaluating lambda 2...\n",
      "\n",
      "\tEvaluating lambda 3...\n",
      "\n",
      "\tEvaluating lambda 4...\n",
      "\n",
      "\tEvaluating lambda 5...\n",
      "\n",
      "\tEvaluating lambda 6...\n",
      "\n",
      "\tEvaluating lambda 7...\n",
      "\n",
      "\tEvaluating lambda 8...\n",
      "\n",
      "\tEvaluating lambda 9...\n",
      "\n",
      "\tEvaluating lambda 10...\n",
      "\n",
      "\tEvaluating lambda 11...\n",
      "\n",
      "\tEvaluating lambda 12...\n",
      "\n",
      "\tEvaluating lambda 13...\n",
      "\n",
      "\tEvaluating lambda 14...\n",
      "\n",
      "\tEvaluating lambda 15...\n",
      "\n",
      "\tEvaluating lambda 16...\n",
      "\n",
      "\tEvaluating lambda 17...\n",
      "\n",
      "\tEvaluating lambda 18...\n",
      "\n",
      "\tEvaluating lambda 19...\n",
      "\n",
      "Running Cross-Validation on Model: 4\n",
      "\tEvaluating lambda 0...\n",
      "\n",
      "\tEvaluating lambda 1...\n",
      "\n",
      "\tEvaluating lambda 2...\n",
      "\n",
      "\tEvaluating lambda 3...\n",
      "\n",
      "\tEvaluating lambda 4...\n",
      "\n",
      "\tEvaluating lambda 5...\n",
      "\n",
      "\tEvaluating lambda 6...\n",
      "\n",
      "\tEvaluating lambda 7...\n",
      "\n",
      "\tEvaluating lambda 8...\n",
      "\n",
      "\tEvaluating lambda 9...\n",
      "\n",
      "\tEvaluating lambda 10...\n",
      "\n",
      "\tEvaluating lambda 11...\n",
      "\n",
      "\tEvaluating lambda 12...\n",
      "\n",
      "\tEvaluating lambda 13...\n",
      "\n",
      "\tEvaluating lambda 14...\n",
      "\n",
      "\tEvaluating lambda 15...\n",
      "\n",
      "\tEvaluating lambda 16...\n",
      "\n",
      "\tEvaluating lambda 17...\n",
      "\n",
      "\tEvaluating lambda 18...\n",
      "\n",
      "\tEvaluating lambda 19...\n",
      "\n",
      "Running Cross-Validation on Model: 5\n",
      "\tEvaluating lambda 0...\n",
      "\n",
      "\tEvaluating lambda 1...\n",
      "\n",
      "\tEvaluating lambda 2...\n",
      "\n",
      "\tEvaluating lambda 3...\n",
      "\n",
      "\tEvaluating lambda 4...\n",
      "\n",
      "\tEvaluating lambda 5...\n",
      "\n",
      "\tEvaluating lambda 6...\n",
      "\n",
      "\tEvaluating lambda 7...\n",
      "\n",
      "\tEvaluating lambda 8...\n",
      "\n",
      "\tEvaluating lambda 9...\n",
      "\n",
      "\tEvaluating lambda 10...\n",
      "\n",
      "\tEvaluating lambda 11...\n",
      "\n",
      "\tEvaluating lambda 12...\n",
      "\n",
      "\tEvaluating lambda 13...\n",
      "\n",
      "\tEvaluating lambda 14...\n",
      "\n",
      "\tEvaluating lambda 15...\n",
      "\n",
      "\tEvaluating lambda 16...\n",
      "\n",
      "\tEvaluating lambda 17...\n",
      "\n",
      "\tEvaluating lambda 18...\n",
      "\n",
      "\tEvaluating lambda 19...\n",
      "\n",
      "Running Cross-Validation on Model: 6\n",
      "\tEvaluating lambda 0...\n",
      "\n",
      "\tEvaluating lambda 1...\n",
      "\n",
      "\tEvaluating lambda 2...\n",
      "\n",
      "\tEvaluating lambda 3...\n",
      "\n",
      "\tEvaluating lambda 4...\n",
      "\n",
      "\tEvaluating lambda 5...\n",
      "\n",
      "\tEvaluating lambda 6...\n",
      "\n",
      "\tEvaluating lambda 7...\n",
      "\n",
      "\tEvaluating lambda 8...\n",
      "\n",
      "\tEvaluating lambda 9...\n",
      "\n",
      "\tEvaluating lambda 10...\n",
      "\n",
      "\tEvaluating lambda 11...\n",
      "\n",
      "\tEvaluating lambda 12...\n",
      "\n",
      "\tEvaluating lambda 13...\n",
      "\n",
      "\tEvaluating lambda 14...\n",
      "\n",
      "\tEvaluating lambda 15...\n",
      "\n",
      "\tEvaluating lambda 16...\n",
      "\n",
      "\tEvaluating lambda 17...\n",
      "\n",
      "\tEvaluating lambda 18...\n",
      "\n",
      "\tEvaluating lambda 19...\n",
      "\n",
      "Running Cross-Validation on Model: 7\n",
      "\tEvaluating lambda 0...\n",
      "\n",
      "\tEvaluating lambda 1...\n",
      "\n",
      "\tEvaluating lambda 2...\n",
      "\n",
      "\tEvaluating lambda 3...\n",
      "\n",
      "\tEvaluating lambda 4...\n",
      "\n",
      "\tEvaluating lambda 5...\n",
      "\n",
      "\tEvaluating lambda 6...\n",
      "\n",
      "\tEvaluating lambda 7...\n",
      "\n",
      "\tEvaluating lambda 8...\n",
      "\n",
      "\tEvaluating lambda 9...\n",
      "\n",
      "\tEvaluating lambda 10...\n",
      "\n",
      "\tEvaluating lambda 11...\n",
      "\n",
      "\tEvaluating lambda 12...\n",
      "\n",
      "\tEvaluating lambda 13...\n",
      "\n",
      "\tEvaluating lambda 14...\n",
      "\n",
      "\tEvaluating lambda 15...\n",
      "\n",
      "\tEvaluating lambda 16...\n",
      "\n",
      "\tEvaluating lambda 17...\n",
      "\n",
      "\tEvaluating lambda 18...\n",
      "\n",
      "\tEvaluating lambda 19...\n",
      "\n",
      "Running Cross-Validation on Model: 8\n",
      "\tEvaluating lambda 0...\n",
      "\n",
      "\tEvaluating lambda 1...\n",
      "\n",
      "\tEvaluating lambda 2...\n",
      "\n",
      "\tEvaluating lambda 3...\n",
      "\n",
      "\tEvaluating lambda 4...\n",
      "\n",
      "\tEvaluating lambda 5...\n",
      "\n",
      "\tEvaluating lambda 6...\n",
      "\n",
      "\tEvaluating lambda 7...\n",
      "\n",
      "\tEvaluating lambda 8...\n",
      "\n",
      "\tEvaluating lambda 9...\n",
      "\n",
      "\tEvaluating lambda 10...\n",
      "\n",
      "\tEvaluating lambda 11...\n",
      "\n",
      "\tEvaluating lambda 12...\n",
      "\n",
      "\tEvaluating lambda 13...\n",
      "\n",
      "\tEvaluating lambda 14...\n",
      "\n",
      "\tEvaluating lambda 15...\n",
      "\n",
      "\tEvaluating lambda 16...\n",
      "\n",
      "\tEvaluating lambda 17...\n",
      "\n",
      "\tEvaluating lambda 18...\n",
      "\n",
      "\tEvaluating lambda 19...\n",
      "\n",
      "Running Cross-Validation on Model: 9\n",
      "\tEvaluating lambda 0...\n",
      "\n",
      "\tEvaluating lambda 1...\n",
      "\n",
      "\tEvaluating lambda 2...\n",
      "\n",
      "\tEvaluating lambda 3...\n",
      "\n",
      "\tEvaluating lambda 4...\n",
      "\n",
      "\tEvaluating lambda 5...\n",
      "\n",
      "\tEvaluating lambda 6...\n",
      "\n",
      "\tEvaluating lambda 7...\n",
      "\n",
      "\tEvaluating lambda 8...\n",
      "\n",
      "\tEvaluating lambda 9...\n",
      "\n",
      "\tEvaluating lambda 10...\n",
      "\n",
      "\tEvaluating lambda 11...\n",
      "\n",
      "\tEvaluating lambda 12...\n",
      "\n",
      "\tEvaluating lambda 13...\n",
      "\n",
      "\tEvaluating lambda 14...\n",
      "\n",
      "\tEvaluating lambda 15...\n",
      "\n",
      "\tEvaluating lambda 16...\n",
      "\n",
      "\tEvaluating lambda 17...\n",
      "\n",
      "\tEvaluating lambda 18...\n",
      "\n",
      "\tEvaluating lambda 19...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run 5-fold CV on each of the 10 models\n",
    "# Takes FOREVER!!!\n",
    "\n",
    "lambdas_opt = np.zeros(num_digits)\n",
    "\n",
    "for i in range(num_digits):\n",
    "    print('Running Cross-Validation on Model:', i)\n",
    "    res = k_fold_CV(X_train, y_train_mat[:, i], lambdas, k)\n",
    "    lambdas_opt[i] = lambdas[np.argmax(np.mean(res, axis=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "latter-ukraine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal lambda for the 0 classifier is equal to: 0.0069519279617756054\n",
      "The optimal lambda for the 1 classifier is equal to: 0.07847599703514611\n",
      "The optimal lambda for the 2 classifier is equal to: 0.018329807108324356\n",
      "The optimal lambda for the 3 classifier is equal to: 0.001623776739188721\n",
      "The optimal lambda for the 4 classifier is equal to: 0.0069519279617756054\n",
      "The optimal lambda for the 5 classifier is equal to: 0.07847599703514611\n",
      "The optimal lambda for the 6 classifier is equal to: 0.12742749857031335\n",
      "The optimal lambda for the 7 classifier is equal to: 0.12742749857031335\n",
      "The optimal lambda for the 8 classifier is equal to: 0.004281332398719396\n",
      "The optimal lambda for the 9 classifier is equal to: 0.018329807108324356\n"
     ]
    }
   ],
   "source": [
    "# Print out each classifier's optimal choice of lambda\n",
    "for i in range(num_digits):\n",
    "    print('The optimal lambda for the', i, 'classifier is equal to:', lambdas_opt[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "outstanding-haven",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for classifier: 0 ...\n",
      "\n",
      "Training for classifier: 1 ...\n",
      "\n",
      "Training for classifier: 2 ...\n",
      "\n",
      "Training for classifier: 3 ...\n",
      "\n",
      "Training for classifier: 4 ...\n",
      "\n",
      "Training for classifier: 5 ...\n",
      "\n",
      "Training for classifier: 6 ...\n",
      "\n",
      "Training for classifier: 7 ...\n",
      "\n",
      "Training for classifier: 8 ...\n",
      "\n",
      "Training for classifier: 9 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrain each classifier with respective optimal value of lambda\n",
    "\n",
    "tolerance = 1\n",
    "classifiers = []\n",
    "for i in range(num_digits):\n",
    "    print('Training for classifier:', i, '...\\n')\n",
    "    lambda_ = lambdas_opt[i]\n",
    "    classifiers.append(fast_grad_algo(eta_init, tolerance, gram, y_train_mat[:,i], lambda_)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-bottle",
   "metadata": {},
   "source": [
    "## (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "fabulous-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct gram matrix for test set\n",
    "\n",
    "gram_test = np.array([kernel_eval(kern_poly, X_train, x_star, offset=1, order=7) for x_star in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "aware-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction matrix\n",
    "\n",
    "preds = np.zeros(gram_test.shape[0])\n",
    "for i, test_vec in enumerate(gram_test):\n",
    "    class_pred = []\n",
    "    for clf in classifiers:\n",
    "        class_pred.append(np.dot(clf.T, test_vec))\n",
    "    preds[i] = np.argmax(class_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "solved-makeup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model test set accuracy: 0.9722222222222222\n",
      "Final model test set misclassification error: 0.02777777777777779\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "\n",
    "print('Final model test set accuracy:', np.mean(preds == y_test))\n",
    "print('Final model test set misclassification error:', 1-np.mean(preds == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-following",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "deadly-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define alternate kernel function\n",
    "\n",
    "def kern_linear(x, y):\n",
    "    return np.dot(x.T, y)\n",
    "\n",
    "def kern_gaussian(x, y, scale):\n",
    "    return np.exp(-scale*np.linalg.norm(x-y, ord=2)**2)\n",
    "\n",
    "def compute_gram(kernel, X, *args):\n",
    "    res = np.zeros((X.shape[0], X.shape[0]))\n",
    "    for i, x_i in enumerate(X):\n",
    "        for j, x_j in enumerate(X):\n",
    "            res[i, j] = kernel(x_i, x_j, *args)\n",
    "    return res\n",
    "\n",
    "def kernel_eval(kernel, X, x_star, *args):\n",
    "    res = np.zeros(X.shape[0])\n",
    "    for i, x in enumerate(X):\n",
    "        res[i] = kernel(x, x_star, *args)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "obvious-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gram matrices on training data, arbitrary scale value chosen\n",
    "scale = 10\n",
    "\n",
    "gram_linear = compute_gram(kern_linear, X_train)\n",
    "gram_gaussian = compute_gram(kern_gaussian, X_train, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "relative-first",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for classifiers: 0 ...\n",
      "\n",
      "Training for classifiers: 1 ...\n",
      "\n",
      "Training for classifiers: 2 ...\n",
      "\n",
      "Training for classifiers: 3 ...\n",
      "\n",
      "Training for classifiers: 4 ...\n",
      "\n",
      "Training for classifiers: 5 ...\n",
      "\n",
      "Training for classifiers: 6 ...\n",
      "\n",
      "Training for classifiers: 7 ...\n",
      "\n",
      "Training for classifiers: 8 ...\n",
      "\n",
      "Training for classifiers: 9 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do not intend on running cross-validation again, will use generic value of lambda = .05\n",
    "\n",
    "tolerance = 1\n",
    "classifiers_linear = []\n",
    "classifiers_gaussian = []\n",
    "for i in range(num_digits):\n",
    "    print('Training for classifiers:', i, '...\\n')\n",
    "    lambda_ = .05\n",
    "    classifiers_linear.append(fast_grad_algo(eta_init, tolerance, gram_linear, y_train_mat[:,i], lambda_)[-1])\n",
    "    classifiers_gaussian.append(fast_grad_algo(eta_init, tolerance, gram_gaussian, y_train_mat[:,i], lambda_)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "binary-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction matrices\n",
    "\n",
    "gram_test_linear = np.array([kernel_eval(kern_linear, X_train, x_star) for x_star in X_test])\n",
    "preds_linear = np.zeros(gram_test.shape[0])\n",
    "\n",
    "for i, test_vec in enumerate(gram_test_linear):\n",
    "    class_pred_linear = []\n",
    "    for clf in classifiers_linear:\n",
    "        class_pred_linear.append(np.dot(clf.T, test_vec))\n",
    "    preds_linear[i] = np.argmax(class_pred_linear)\n",
    "    \n",
    "    \n",
    "gram_test_gaussian = np.array([kernel_eval(kern_gaussian, X_train, x_star, scale) for x_star in X_test])\n",
    "preds_gaussian = np.zeros(gram_test.shape[0])\n",
    "\n",
    "for i, test_vec in enumerate(gram_test_gaussian):\n",
    "    class_pred_gaussian = []\n",
    "    for clf in classifiers_gaussian:\n",
    "        class_pred_gaussian.append(np.dot(clf.T, test_vec))\n",
    "    preds_gaussian[i] = np.argmax(class_pred_gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "million-audio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final linear model test set accuracy: 0.4444444444444444\n",
      "Final gaussian model test set accuracy: 0.7222222222222222\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "\n",
    "print('Final linear model test set accuracy:', np.mean(preds_linear == y_test))\n",
    "\n",
    "print('Final gaussian model test set accuracy:', np.mean(preds_gaussian == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-spanking",
   "metadata": {},
   "source": [
    "As can be seen above, the model performance is quite poor when compared to the model that used the polynomial kernel.\n",
    "That being said, this is most likely due to a poor tuning of parameters/hyper-parameters. With a little more time and effort, I believe both the linear kernel and the gaussian kernel could produce models with much higher predictive accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DATA558] *",
   "language": "python",
   "name": "conda-env-DATA558-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
